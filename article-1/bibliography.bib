@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{fei2006one,
  title={One-shot learning of object categories},
  author={Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={28},
  number={4},
  pages={594--611},
  year={2006},
  publisher={IEEE}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{Rougier:2018,
  doi = {10.1038/d41586-018-04628-w},
  year = {2018},
  month = apr,
  publisher = {Springer Nature},
  volume = {556},
  number = {7701},
  pages = {309--309},
  author = {Rougier, Nicolas P. and Hinsen, Konrad},
  title = {Code reviewing puts extra demands on referees },
  journal = {Nature},
}

@article{Science:2018,
  doi = {10.1126/science.359.6377.725},
  author = {Hutson, Matthew},
  url = {http://science.sciencemag.org/content/359/6377/725},
  year = {2018},
  month = feb,
  volume = {359},
  number = {6377},
  title = {Artificial intelligence faces a replication crisis},
  journal = {Science},
}

@Misc{Roboto:2011,
  author =       {Christian Robertson},
  title =        {The Roboto family of fonts (Google)},
  url =          {https://github.com/google/roboto},
  year =         2011,
  note =         {Apache License, verison 2.0},
}

@Misc{SourceSerifPro:2014,
  author =       {Frank Grießhammer},
  title =        {Source Serif Pro (Adobe Systems)},
  url =          {https://github.com/adobe-fonts/source-serif-pro},
  year =         2014,
  note =         {SIL Open Font License, version 1.1},
}

@Misc{SourceCodePro:2012,
  author =       {Paul D. Hunt},
  title =        {Source Code Pro (Adobe Systems)},
  url =          {https://github.com/adobe-fonts/source-code-pro},
  year =         2012,
  note =         {SIL Open Font License, version 1.1},
}

@article{Topalidou:2015,
  author =       {Topalidou, Meropi and Rougier, Nicolas P.},
  title =        {{[Re] Interaction between cognitive and motor cortico-basal
                  ganglia loops during decision making: a computational study}},
  journal =      {ReScience},
  year =         2015,
  volume =       1,
  number =       1,
  doi =          {10.5281/zenodo.27944},
}

@article{Rougier:2017,
  doi =          {10.7717/peerj-cs.142},
  author =       {Nicolas P. Rougier and Konrad Hinsen and Frédéric Alexandre
                  and Thomas Arildsen and Lorena Barba and Fabien
                  C. Y. Benureau and C. Titus Brown and Pierre de Buyl and Ozan
                  Caglayan and Andrew P. Davison and Marc André Delsuc and
                  Georgios Detorakis and Alexandra K. Diem and Damien Drix and
                  Pierre Enel and Benoît Girard and Olivia Guest and Matt
                  G. Hall and Rafael Neto Henriques and Xavier Hinaut and Kamil
                  S Jaron and Mehdi Khamassi and Almar Klein and Tiina Manninen
                  and Pietro Marchesi and Dan McGlinn and Christoph Metzner and
                  Owen L. Petchey and Hans Ekkehard Plesser and Timothée Poisot
                  and Karthik Ram and Yoav Ram and Etienne Roesch and Cyrille
                  Rossant and Vahid Rostami and Aaron Shifman and Joseph
                  Stachelek and Marcel Stimberg and Frank Stollmeier and
                  Federico Vaggi and Guillaume Viejo and Julien Vitay and Anya
                  Vostinar and Roman Yurchak and Tiziano Zito},
  title =        {{Sustainable computational science: the ReScience
                  initiative}},
  journal =      {{PeerJ} Computer Science},
  month =        12,
  volume =       3,
  pages =        {e142},
  year =         2017,
  github =       {https://github.com/ReScience/ReScience-article-2},
  keywords =     {journal},
  tags =         {OS},
}

@Article{doc,
  author = 	 {Adam-Bourdarios, Claire and  Cowan, Glen  and Germain, Cécile  and Guyon, Isabelle and Kegl, Balazs and Rousseau, David },
  title = 	 {Learning to discover: the Higgs boson machine learning challenge},
  journal = 	 {LAL-Orsay},
  year = 	 2014,
}
@article{dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@misc{ICLR18,
	title = {{ICLR} 2018 {Reproducibility} {Challenge}},
	url = {https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html},
	urldate = {2018-12-02},
	file = {ICLR 2018 Reproducibility Challenge:/Users/sylvainchatel/Zotero/storage/NQ8M8EXZ/ICLR2018-ReproducibilityChallenge.html:text/html}
}

@misc{ICLR19,
	title = {{ICLR} {Reproducibility} {Challenge}},
	url = {https://reproducibility-challenge.github.io/iclr_2019/},
	abstract = {Second Edition, 2019},
	language = {en-US},
	urldate = {2018-12-02},
	journal = {ICLR Reproducibility Challenge},
	file = {Snapshot:/Users/sylvainchatel/Zotero/storage/PZSQ68YC/iclr_2019.html:text/html}
}

@misc{JP,
	title = {Joelle {Pineau}: {Reproducibility}, {Reusability}, and {Robustness} in {Deep} {Reinforcement} {Learning} {ICLR} 2018},
	shorttitle = {Joelle {Pineau}},
	url = {https://www.youtube.com/watch?v=Vh4H0gOwdIg},
	urldate = {2018-12-02},
	author = {{Steven Van Vaerenbergh}}
}

@article{reptile,
  title={Reptile: a Scalable Metalearning Algorithm},
  author={Nichol, Alex and Schulman, John},
  journal={arXiv preprint arXiv:1803.02999},
  year={2018}
}

@article{dermato,
  title={Dermatologist-level classification of skin cancer with deep neural networks},
  author={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
  journal={Nature},
  volume={542},
  number={7639},
  pages={115},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{alphago,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{vinyals,
	title = {Matching {Networks} for {One} {Shot} {Learning}},
	url = {http://arxiv.org/abs/1606.04080},
	abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6\% to 93.2\% and from 88.0\% to 93.8\% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
	urldate = {2018-12-03},
	journal = {arXiv:1606.04080 [cs, stat]},
	author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04080},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1606.04080 PDF:/Users/sylvainchatel/Zotero/storage/H3TCJCTB/Vinyals et al. - 2016 - Matching Networks for One Shot Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/sylvainchatel/Zotero/storage/ASFZNBJI/1606.html:text/html}
}

@article{finn,
	title = {Model-{Agnostic} {Meta}-{Learning} for {Fast} {Adaptation} of {Deep} {Networks}},
	url = {http://arxiv.org/abs/1703.03400},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	urldate = {2018-12-03},
	journal = {ICML},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.03400},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: ICML 2017. Code at https://github.com/cbfinn/maml, Videos of RL results at https://sites.google.com/view/maml, Blog post at http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/},
	file = {arXiv\:1703.03400 PDF:/Users/sylvainchatel/Zotero/storage/IFLS99VX/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf:application/pdf;arXiv.org Snapshot:/Users/sylvainchatel/Zotero/storage/37VW4JMD/1703.html:text/html}
}

@article{ravi,
	title = {Optimization as a model for few-shot learning},
	abstract = {Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a classiﬁer has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity classiﬁers requires many iterative steps over many examples to perform well. Here, we propose an LSTMbased meta-learner model to learn the exact optimization algorithm used to train another learner neural network classiﬁer in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates speciﬁcally for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner (classiﬁer) network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning.},
	language = {en},
	author = {Ravi, Sachin and Larochelle, Hugo},
	journal = {ICLR},
	year = {2017},
	pages = {11},
	file = {Ravi and Larochelle - 2017 - OPTIMIZATION AS A MODEL FOR FEW-SHOT LEARNING.pdf:/Users/sylvainchatel/Zotero/storage/HBM43HKC/Ravi and Larochelle - 2017 - OPTIMIZATION AS A MODEL FOR FEW-SHOT LEARNING.pdf:application/pdf}
}

@article{snell,
	title = {Prototypical {Networks} for {Few}-shot {Learning}},
	url = {http://arxiv.org/abs/1703.05175},
	abstract = {We propose prototypical networks for the problem of few-shot classiﬁcation, where a classiﬁer must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classiﬁcation can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reﬂect a simpler inductive bias that is beneﬁcial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-theart results on the CU-Birds dataset.},
	language = {en},
	urldate = {2018-12-03},
	journal = {arXiv:1703.05175 [cs, stat]},
	author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.05175},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Snell et al. - 2017 - Prototypical Networks for Few-shot Learning.pdf:/Users/sylvainchatel/Zotero/storage/YDN2THC7/Snell et al. - 2017 - Prototypical Networks for Few-shot Learning.pdf:application/pdf}
}

@article{mishra,
	title = {A {Simple} {Neural} {Attentive} {Meta}-{Learner}},
	url = {http://arxiv.org/abs/1707.03141},
	abstract = {Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint speciﬁc pieces of information. In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by signiﬁcant margins.},
	language = {en},
	urldate = {2018-12-03},
	journal = {arXiv: 1707.03141 [cs, stat]},
	author = {Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.03141},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: iclr 2018 version},
	file = {Mishra et al. - 2017 - A Simple Neural Attentive Meta-Learner.pdf:/Users/sylvainchatel/Zotero/storage/2NXY6NXI/Mishra et al. - 2017 - A Simple Neural Attentive Meta-Learner.pdf:application/pdf}
}

@article{garcia,
	title = {Few-{Shot} {Learning} with {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.04043},
	abstract = {We propose to study the problem of few-shot learning with the prism of inference on a partially observed graphical model, constructed from a collection of input images whose label can be either observed or not. By assimilating generic message-passing inference algorithms with their neural-network counterparts, we deﬁne a graph neural network architecture that generalizes several of the recently proposed few-shot learning models. Besides providing improved numerical performance, our framework is easily extended to variants of few-shot learning, such as semi-supervised or active learning, demonstrating the ability of graph-based models to operate well on ‘relational’ tasks.},
	language = {en},
	urldate = {2018-12-03},
	journal = {arXiv:1711.04043 [cs, stat]},
	author = {Garcia, Victor and Bruna, Joan},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.04043},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Garcia and Bruna - 2017 - Few-Shot Learning with Graph Neural Networks.pdf:/Users/sylvainchatel/Zotero/storage/ELW77VPA/Garcia and Bruna - 2017 - Few-Shot Learning with Graph Neural Networks.pdf:application/pdf}
}

@article{mhaskar2016learning,
  title={Learning functions: when is deep better than shallow},
  author={Mhaskar, Hrushikesh and Liao, Qianli and Poggio, Tomaso},
  journal={arXiv preprint arXiv:1603.00988},
  year={2016}
}

@misc{GNN,
	title = {Contribute to vgsatorras/few-shot-gnn development by creating an account on {GitHub}},
	url = {https://github.com/vgsatorras/few-shot-gnn},
	urldate = {2018-12-03},
	author = {vgsatorras},
	month = dec,
	year = {2018},
	note = {original-date: 2017-11-09T17:01:12Z}
}

@misc{SNAIL,
	title = {A {PyTorch} implementation of the blocks from the \_A {Simple} {Neural} {Attentive} {Meta}-{Learner}\_ paper: sagelywizard/snail},
	copyright = {MIT},
	shorttitle = {A {PyTorch} implementation of the blocks from the \_A {Simple} {Neural} {Attentive} {Meta}-{Learner}\_ paper},
	url = {https://github.com/sagelywizard/snail},
	urldate = {2018-12-03},
	author = {Bastian, Benjamin},
	month = nov,
	year = {2018},
	note = {original-date: 2018-01-19T04:39:46Z}
}

@misc{PROTO,
	title = {Code for the {NIPS} 2017 {Paper} "{Prototypical} {Networks} for {Few}-shot {Learning}": jakesnell/prototypical-networks},
	copyright = {MIT},
	shorttitle = {Code for the {NIPS} 2017 {Paper} "{Prototypical} {Networks} for {Few}-shot {Learning}"},
	url = {https://github.com/jakesnell/prototypical-networks},
	urldate = {2018-12-03},
	author = {Snell, Jake},
	month = dec,
	year = {2018},
	note = {original-date: 2017-11-05T04:38:19Z}
}

@misc{MAML,
	title = {Code for "{Model}-{Agnostic} {Meta}-{Learning} for {Fast} {Adaptation} of {Deep} {Networks}": cbfinn/maml},
	copyright = {MIT},
	shorttitle = {Code for "{Model}-{Agnostic} {Meta}-{Learning} for {Fast} {Adaptation} of {Deep} {Networks}"},
	url = {https://github.com/cbfinn/maml},
	urldate = {2018-12-03},
	author = {Finn, Chelsea},
	month = dec,
	year = {2018},
	note = {original-date: 2017-06-18T01:36:06Z}
}

@inproceedings{tensorflow,
  title={Tensorflow: a system for large-scale machine learning.},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={OSDI},
  volume={16},
  pages={265--283},
  year={2016}
}

@inproceedings{
R2D2,
title={Meta-learning with differentiable closed-form solvers},
author={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HyxnZh0ct7},
}

@misc{metaLSTM,
	title = {This repo contains the source code accompanying a scientific paper with the same name.: twitter/meta-learning-lstm},
	copyright = {MIT},
	shorttitle = {This repo contains the source code accompanying a scientific paper with the same name.},
	url = {https://github.com/twitter/meta-learning-lstm},
	urldate = {2018-12-03},
	publisher = {Twitter, Inc.},
	month = nov,
	year = {2018},
	note = {original-date: 2016-10-11T06:57:42Z}
}

@misc{MatchingNet,
	title = {This repo provides pytorch code which replicates the results of the {Matching} {Networks} for {One} {Shot} {Learning} paper on the {Omniglot} and {MiniImageNet} dataset: gitabcworld/{MatchingNetworks}},
	copyright = {View license},
	shorttitle = {This repo provides pytorch code which replicates the results of the {Matching} {Networks} for {One} {Shot} {Learning} paper on the {Omniglot} and {MiniImageNet} dataset},
	url = {https://github.com/gitabcworld/MatchingNetworks},
	urldate = {2018-12-03},
	author = {Centeno, Albert Berenguel},
	month = dec,
	year = {2018},
	note = {original-date: 2017-06-29T07:43:16Z}
}


@article{R2D2_old,
	title = {Meta-learning with differentiable closed-form solvers},
	url = {https://openreview.net/forum?id=HyxnZh0ct7},
	abstract = {Adapting deep networks to new concepts from few examples is challenging, due to the high computational and data requirements of standard fine-tuning procedures.
  Most work on few-shot learning has...},
	urldate = {2018-12-03},
	author = {Anonymous},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/Users/sylvainchatel/Zotero/storage/VDQVNQ22/Anonymous - 2018 - Meta-learning with differentiable closed-form solv.pdf:application/pdf;Snapshot:/Users/sylvainchatel/Zotero/storage/XJFW5W6X/forum.html:text/html}
}
